{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Superhero Attributes and Power Classification\n",
                "\n",
                "**DSCI 4411 - Fundamentals of Data Mining**  \n",
                "**The American University in Cairo - Fall 2025**\n",
                "\n",
                "This project explores classification and clustering techniques using a dataset of superheroes with various attributes such as powers, skills, physical traits, and biography data.\n",
                "\n",
                "## Enhanced Analysis with Multiple Models & Techniques"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Install additional packages if needed\n",
                "# !pip install xgboost lightgbm catboost\n",
                "\n",
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.pipeline import make_pipeline, Pipeline\n",
                "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
                "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, MinMaxScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import (RandomForestClassifier, GradientBoostingClassifier, \n",
                "                              AdaBoostClassifier, ExtraTreesClassifier, \n",
                "                              HistGradientBoostingClassifier, VotingClassifier, StackingClassifier)\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.neighbors import KNeighborsClassifier\n",
                "from sklearn.naive_bayes import GaussianNB\n",
                "from sklearn.tree import DecisionTreeClassifier\n",
                "from sklearn.neural_network import MLPClassifier\n",
                "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, roc_auc_score\n",
                "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
                "from sklearn.decomposition import PCA\n",
                "from sklearn.metrics import silhouette_score\n",
                "from sklearn.feature_selection import SelectKBest, f_classif, RFE\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Try to import XGBoost (optional)\n",
                "try:\n",
                "    from xgboost import XGBClassifier\n",
                "    HAS_XGB = True\n",
                "except ImportError:\n",
                "    HAS_XGB = False\n",
                "    print(\"XGBoost not installed, skipping...\")\n",
                "\n",
                "# Set style for visualizations\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "plt.rcParams['figure.figsize'] = (12, 7)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "df = pd.read_csv('superhero dataset.csv')\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
                "print(f\"\\nTarget Distribution:\")\n",
                "print(df['is_good'].value_counts(normalize=True))\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create figures directory\n",
                "import os\n",
                "os.makedirs('figures', exist_ok=True)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Statistical summary\n",
                "print(\"Statistical Summary:\")\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target variable distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "colors = ['#e74c3c', '#2ecc71']\n",
                "df['is_good'].value_counts().plot(kind='pie', ax=axes[0], colors=colors, \n",
                "                                   labels=['Villain', 'Hero'], autopct='%1.1f%%')\n",
                "axes[0].set_title('Target Distribution', fontsize=14)\n",
                "\n",
                "sns.countplot(data=df, x='is_good', palette=colors, ax=axes[1])\n",
                "axes[1].set_xticklabels(['Villain (0)', 'Hero (1)'])\n",
                "axes[1].set_title('Villains vs Heroes Count', fontsize=14)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/target_distribution.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Power distribution analysis\n",
                "power_cols = ['super_strength', 'flight', 'energy_projection', 'telepathy', \n",
                "              'healing_factor', 'shape_shifting', 'invisibility', 'telekinesis']\n",
                "\n",
                "# Compare powers between heroes and villains\n",
                "hero_powers = df[df['is_good'] == 1][power_cols].mean()\n",
                "villain_powers = df[df['is_good'] == 0][power_cols].mean()\n",
                "\n",
                "fig, ax = plt.subplots(figsize=(12, 6))\n",
                "x = np.arange(len(power_cols))\n",
                "width = 0.35\n",
                "\n",
                "ax.bar(x - width/2, hero_powers * 100, width, label='Heroes', color='#2ecc71')\n",
                "ax.bar(x + width/2, villain_powers * 100, width, label='Villains', color='#e74c3c')\n",
                "\n",
                "ax.set_ylabel('Percentage with Power (%)')\n",
                "ax.set_title('Power Distribution: Heroes vs Villains')\n",
                "ax.set_xticks(x)\n",
                "ax.set_xticklabels(power_cols, rotation=45, ha='right')\n",
                "ax.legend()\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/power_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap\n",
                "plt.figure(figsize=(14, 12))\n",
                "corr = df.corr()\n",
                "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
                "sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', cmap='RdYlBu_r', center=0, \n",
                "            square=True, linewidths=0.5)\n",
                "plt.title('Feature Correlation Heatmap', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/correlation_heatmap.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Advanced Preprocessing & Feature Engineering"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create new features through feature engineering\n",
                "df_engineered = df.copy()\n",
                "\n",
                "# Total number of powers\n",
                "df_engineered['total_powers'] = df_engineered[power_cols].sum(axis=1)\n",
                "\n",
                "# Power level per year active (efficiency)\n",
                "df_engineered['power_efficiency'] = df_engineered['power_level'] / (df_engineered['years_active'] + 1)\n",
                "\n",
                "# Training intensity (hours per week / age)\n",
                "df_engineered['training_intensity'] = df_engineered['training_hours_per_week'] / (df_engineered['age'] + 1)\n",
                "\n",
                "# Casualties ratio (casualties per year active)\n",
                "df_engineered['casualty_rate'] = df_engineered['civilian_casualties_past_year'] / (df_engineered['years_active'] + 1)\n",
                "\n",
                "# Approval to power ratio\n",
                "df_engineered['approval_power_ratio'] = df_engineered['public_approval_rating'] / (df_engineered['power_level'] + 1)\n",
                "\n",
                "# BMI (body mass index)\n",
                "df_engineered['bmi'] = df_engineered['weight_kg'] / ((df_engineered['height_cm'] / 100) ** 2)\n",
                "\n",
                "# Experience score\n",
                "df_engineered['experience_score'] = df_engineered['years_active'] * df_engineered['training_hours_per_week']\n",
                "\n",
                "print(\"New features created:\")\n",
                "new_features = ['total_powers', 'power_efficiency', 'training_intensity', \n",
                "                'casualty_rate', 'approval_power_ratio', 'bmi', 'experience_score']\n",
                "print(new_features)\n",
                "print(f\"\\nNew dataset shape: {df_engineered.shape}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features and target\n",
                "X = df_engineered.drop('is_good', axis=1)\n",
                "y = df_engineered['is_good']\n",
                "\n",
                "# Train/test split with stratification\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.2, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Training set: {len(X_train)} samples\")\n",
                "print(f\"Test set: {len(X_test)} samples\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature scaling\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"Features scaled using StandardScaler\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature Selection using SelectKBest\n",
                "selector = SelectKBest(f_classif, k=10)\n",
                "X_train_selected = selector.fit_transform(X_train_scaled, y_train)\n",
                "X_test_selected = selector.transform(X_test_scaled)\n",
                "\n",
                "# Get selected feature names\n",
                "selected_mask = selector.get_support()\n",
                "selected_features = X.columns[selected_mask].tolist()\n",
                "print(f\"Top 10 features by ANOVA F-value:\")\n",
                "print(selected_features)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Comprehensive Classification - Multiple Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Define all models to test\n",
                "models = {\n",
                "    # Linear Models\n",
                "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
                "    'LDA': LinearDiscriminantAnalysis(),\n",
                "    \n",
                "    # Tree-based Models\n",
                "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
                "    'Random Forest': RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
                "    'Extra Trees': ExtraTreesClassifier(n_estimators=200, random_state=42, n_jobs=-1),\n",
                "    'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
                "    'HistGradientBoosting': HistGradientBoostingClassifier(random_state=42),\n",
                "    'AdaBoost': AdaBoostClassifier(random_state=42),\n",
                "    \n",
                "    # Instance-based\n",
                "    'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
                "    'KNN (k=10)': KNeighborsClassifier(n_neighbors=10),\n",
                "    \n",
                "    # SVM variants\n",
                "    'SVM (Linear)': SVC(kernel='linear', random_state=42),\n",
                "    'SVM (RBF)': SVC(kernel='rbf', random_state=42),\n",
                "    'SVM (Poly)': SVC(kernel='poly', degree=3, random_state=42),\n",
                "    \n",
                "    # Probabilistic\n",
                "    'Naive Bayes': GaussianNB(),\n",
                "    'QDA': QuadraticDiscriminantAnalysis(),\n",
                "    \n",
                "    # Neural Network\n",
                "    'MLP (small)': MLPClassifier(hidden_layer_sizes=(50,), max_iter=500, random_state=42),\n",
                "    'MLP (medium)': MLPClassifier(hidden_layer_sizes=(100, 50), max_iter=500, random_state=42),\n",
                "    'MLP (large)': MLPClassifier(hidden_layer_sizes=(100, 100, 50), max_iter=500, random_state=42),\n",
                "}\n",
                "\n",
                "# Add XGBoost if available\n",
                "if HAS_XGB:\n",
                "    models['XGBoost'] = XGBClassifier(n_estimators=200, random_state=42, use_label_encoder=False, \n",
                "                                       eval_metric='logloss', verbosity=0)\n",
                "\n",
                "print(f\"Testing {len(models)} different models...\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluate all models with cross-validation\n",
                "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
                "results = []\n",
                "\n",
                "for name, model in models.items():\n",
                "    try:\n",
                "        # Use scaled data for models that need it\n",
                "        if name in ['Logistic Regression', 'SVM (Linear)', 'SVM (RBF)', 'SVM (Poly)', \n",
                "                    'KNN (k=5)', 'KNN (k=10)', 'MLP (small)', 'MLP (medium)', 'MLP (large)']:\n",
                "            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')\n",
                "            model.fit(X_train_scaled, y_train)\n",
                "            test_pred = model.predict(X_test_scaled)\n",
                "        else:\n",
                "            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')\n",
                "            model.fit(X_train, y_train)\n",
                "            test_pred = model.predict(X_test)\n",
                "        \n",
                "        test_acc = accuracy_score(y_test, test_pred)\n",
                "        f1 = f1_score(y_test, test_pred)\n",
                "        \n",
                "        results.append({\n",
                "            'Model': name,\n",
                "            'CV Mean': cv_scores.mean(),\n",
                "            'CV Std': cv_scores.std(),\n",
                "            'Test Accuracy': test_acc,\n",
                "            'F1 Score': f1\n",
                "        })\n",
                "        \n",
                "        print(f\"{name:25s} | CV: {cv_scores.mean():.4f} ¬± {cv_scores.std():.4f} | Test: {test_acc:.4f} | F1: {f1:.4f}\")\n",
                "    except Exception as e:\n",
                "        print(f\"{name:25s} | Error: {str(e)[:50]}\")\n",
                "\n",
                "# Create results DataFrame\n",
                "results_df = pd.DataFrame(results).sort_values('Test Accuracy', ascending=False)\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"TOP 5 MODELS:\")\n",
                "print(results_df.head())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize model comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(16, 8))\n",
                "\n",
                "# Sort by test accuracy\n",
                "results_sorted = results_df.sort_values('Test Accuracy', ascending=True)\n",
                "\n",
                "# Bar chart of test accuracy\n",
                "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(results_sorted)))\n",
                "axes[0].barh(results_sorted['Model'], results_sorted['Test Accuracy'], color=colors)\n",
                "axes[0].set_xlabel('Test Accuracy')\n",
                "axes[0].set_title('Model Comparison: Test Accuracy', fontsize=14)\n",
                "axes[0].axvline(x=0.65, color='red', linestyle='--', label='Baseline (65%)')\n",
                "\n",
                "# CV accuracy with error bars\n",
                "axes[1].barh(results_sorted['Model'], results_sorted['CV Mean'], \n",
                "             xerr=results_sorted['CV Std'], color=colors, capsize=3)\n",
                "axes[1].set_xlabel('CV Accuracy (with std)')\n",
                "axes[1].set_title('Model Comparison: Cross-Validation Accuracy', fontsize=14)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/model_comparison_all.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Hyperparameter Tuning for Best Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tune Random Forest\n",
                "print(\"Tuning Random Forest...\")\n",
                "rf_params = {\n",
                "    'n_estimators': [100, 200, 300],\n",
                "    'max_depth': [5, 10, 15, None],\n",
                "    'min_samples_split': [2, 5, 10],\n",
                "    'min_samples_leaf': [1, 2, 4]\n",
                "}\n",
                "\n",
                "rf_grid = GridSearchCV(\n",
                "    RandomForestClassifier(random_state=42, n_jobs=-1),\n",
                "    rf_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
                ")\n",
                "rf_grid.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best RF params: {rf_grid.best_params_}\")\n",
                "print(f\"Best RF CV score: {rf_grid.best_score_:.4f}\")\n",
                "print(f\"Best RF test score: {rf_grid.score(X_test, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tune Gradient Boosting\n",
                "print(\"\\nTuning Gradient Boosting...\")\n",
                "gb_params = {\n",
                "    'n_estimators': [100, 200],\n",
                "    'learning_rate': [0.01, 0.1, 0.2],\n",
                "    'max_depth': [3, 5, 7],\n",
                "    'min_samples_split': [2, 5]\n",
                "}\n",
                "\n",
                "gb_grid = GridSearchCV(\n",
                "    GradientBoostingClassifier(random_state=42),\n",
                "    gb_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
                ")\n",
                "gb_grid.fit(X_train, y_train)\n",
                "\n",
                "print(f\"Best GB params: {gb_grid.best_params_}\")\n",
                "print(f\"Best GB CV score: {gb_grid.best_score_:.4f}\")\n",
                "print(f\"Best GB test score: {gb_grid.score(X_test, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Tune SVM\n",
                "print(\"\\nTuning SVM...\")\n",
                "svm_params = {\n",
                "    'C': [0.1, 1, 10, 100],\n",
                "    'gamma': ['scale', 'auto', 0.1, 0.01],\n",
                "    'kernel': ['rbf', 'poly']\n",
                "}\n",
                "\n",
                "svm_grid = GridSearchCV(\n",
                "    SVC(random_state=42),\n",
                "    svm_params, cv=5, scoring='accuracy', n_jobs=-1, verbose=1\n",
                ")\n",
                "svm_grid.fit(X_train_scaled, y_train)\n",
                "\n",
                "print(f\"Best SVM params: {svm_grid.best_params_}\")\n",
                "print(f\"Best SVM CV score: {svm_grid.best_score_:.4f}\")\n",
                "print(f\"Best SVM test score: {svm_grid.score(X_test_scaled, y_test):.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Ensemble Methods"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Voting Classifier (combine best models)\n",
                "print(\"Creating Voting Ensemble...\")\n",
                "\n",
                "voting_clf = VotingClassifier(\n",
                "    estimators=[\n",
                "        ('rf', rf_grid.best_estimator_),\n",
                "        ('gb', gb_grid.best_estimator_),\n",
                "        ('lr', LogisticRegression(max_iter=1000, random_state=42))\n",
                "    ],\n",
                "    voting='hard'\n",
                ")\n",
                "\n",
                "# Need to standardize data for LR in the ensemble\n",
                "voting_cv = cross_val_score(voting_clf, X_train, y_train, cv=5, scoring='accuracy')\n",
                "voting_clf.fit(X_train, y_train)\n",
                "voting_test = voting_clf.score(X_test, y_test)\n",
                "\n",
                "print(f\"Voting Ensemble CV: {voting_cv.mean():.4f} ¬± {voting_cv.std():.4f}\")\n",
                "print(f\"Voting Ensemble Test: {voting_test:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Stacking Classifier\n",
                "print(\"\\nCreating Stacking Ensemble...\")\n",
                "\n",
                "stacking_clf = StackingClassifier(\n",
                "    estimators=[\n",
                "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
                "        ('gb', GradientBoostingClassifier(random_state=42)),\n",
                "        ('knn', KNeighborsClassifier(n_neighbors=5))\n",
                "    ],\n",
                "    final_estimator=LogisticRegression(max_iter=1000),\n",
                "    cv=5\n",
                ")\n",
                "\n",
                "stacking_cv = cross_val_score(stacking_clf, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
                "stacking_clf.fit(X_train_scaled, y_train)\n",
                "stacking_test = stacking_clf.score(X_test_scaled, y_test)\n",
                "\n",
                "print(f\"Stacking Ensemble CV: {stacking_cv.mean():.4f} ¬± {stacking_cv.std():.4f}\")\n",
                "print(f\"Stacking Ensemble Test: {stacking_test:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 7. Best Model Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Identify best model\n",
                "all_results = [\n",
                "    ('RF (Tuned)', rf_grid.best_score_, rf_grid.score(X_test, y_test)),\n",
                "    ('GB (Tuned)', gb_grid.best_score_, gb_grid.score(X_test, y_test)),\n",
                "    ('SVM (Tuned)', svm_grid.best_score_, svm_grid.score(X_test_scaled, y_test)),\n",
                "    ('Voting Ensemble', voting_cv.mean(), voting_test),\n",
                "    ('Stacking Ensemble', stacking_cv.mean(), stacking_test)\n",
                "]\n",
                "\n",
                "best_result = max(all_results, key=lambda x: x[2])\n",
                "print(f\"\\n{'='*60}\")\n",
                "print(f\"BEST MODEL: {best_result[0]}\")\n",
                "print(f\"CV Accuracy: {best_result[1]:.4f}\")\n",
                "print(f\"Test Accuracy: {best_result[2]:.4f}\")\n",
                "print(f\"{'='*60}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance from best RF\n",
                "feature_imp = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'importance': rf_grid.best_estimator_.feature_importances_\n",
                "}).sort_values('importance', ascending=True)\n",
                "\n",
                "plt.figure(figsize=(10, 10))\n",
                "colors = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(feature_imp)))\n",
                "plt.barh(feature_imp['feature'], feature_imp['importance'], color=colors)\n",
                "plt.xlabel('Feature Importance')\n",
                "plt.title('Random Forest: Feature Importance (Tuned)', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/feature_importance_tuned.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nTop 5 Most Important Features:\")\n",
                "print(feature_imp.tail(5))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrix for best model\n",
                "best_pred = rf_grid.predict(X_test)\n",
                "\n",
                "plt.figure(figsize=(8, 6))\n",
                "cm = confusion_matrix(y_test, best_pred)\n",
                "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
                "            xticklabels=['Villain', 'Hero'], yticklabels=['Villain', 'Hero'])\n",
                "plt.xlabel('Predicted')\n",
                "plt.ylabel('Actual')\n",
                "plt.title(f'Confusion Matrix: Best Model (RF Tuned)', fontsize=14)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/confusion_matrix_best.png', dpi=150)\n",
                "plt.show()\n",
                "\n",
                "print(\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, best_pred, target_names=['Villain', 'Hero']))"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 8. Advanced Clustering Analysis"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Select behavioral features for clustering (excluding target)\n",
                "cluster_features = ['power_level', 'civilian_casualties_past_year', 'training_hours_per_week',\n",
                "                    'years_active', 'public_approval_rating', 'total_powers', 'power_efficiency']\n",
                "\n",
                "X_cluster = df_engineered[cluster_features].copy()\n",
                "X_cluster_scaled = StandardScaler().fit_transform(X_cluster)\n",
                "\n",
                "print(f\"Clustering on {len(cluster_features)} behavioral features\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# K-Means with optimal k selection\n",
                "sil_scores = []\n",
                "k_range = range(2, 10)\n",
                "\n",
                "for k in k_range:\n",
                "    km = KMeans(n_clusters=k, random_state=42, n_init=20)\n",
                "    labels = km.fit_predict(X_cluster_scaled)\n",
                "    sil = silhouette_score(X_cluster_scaled, labels)\n",
                "    sil_scores.append(sil)\n",
                "    print(f\"k={k}: Silhouette Score = {sil:.4f}\")\n",
                "\n",
                "best_k = k_range[np.argmax(sil_scores)]\n",
                "print(f\"\\nBest k = {best_k} with Silhouette = {max(sil_scores):.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Apply best K-Means\n",
                "kmeans_best = KMeans(n_clusters=best_k, random_state=42, n_init=20)\n",
                "df_engineered['kmeans_cluster'] = kmeans_best.fit_predict(X_cluster_scaled)\n",
                "\n",
                "print(f\"K-Means Clusters (k={best_k}):\")\n",
                "print(df_engineered['kmeans_cluster'].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Try DBSCAN clustering\n",
                "print(\"\\nTesting DBSCAN...\")\n",
                "for eps in [0.5, 1.0, 1.5, 2.0]:\n",
                "    for min_samples in [3, 5, 10]:\n",
                "        dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
                "        labels = dbscan.fit_predict(X_cluster_scaled)\n",
                "        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)\n",
                "        n_noise = list(labels).count(-1)\n",
                "        if n_clusters >= 2 and n_clusters <= 10:\n",
                "            sil = silhouette_score(X_cluster_scaled[labels != -1], labels[labels != -1]) if sum(labels != -1) > 1 else 0\n",
                "            print(f\"eps={eps}, min_samples={min_samples}: {n_clusters} clusters, {n_noise} noise, Silhouette={sil:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Hierarchical Clustering\n",
                "print(\"\\nTesting Agglomerative Clustering...\")\n",
                "for n_clusters in [2, 3, 4, 5]:\n",
                "    for linkage in ['ward', 'complete', 'average']:\n",
                "        agg = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage)\n",
                "        labels = agg.fit_predict(X_cluster_scaled)\n",
                "        sil = silhouette_score(X_cluster_scaled, labels)\n",
                "        print(f\"n={n_clusters}, linkage={linkage}: Silhouette = {sil:.4f}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PCA Visualization of clusters\n",
                "pca = PCA(n_components=2)\n",
                "X_pca = pca.fit_transform(X_cluster_scaled)\n",
                "\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
                "\n",
                "# K-Means clusters\n",
                "scatter1 = axes[0].scatter(X_pca[:, 0], X_pca[:, 1], c=df_engineered['kmeans_cluster'], \n",
                "                          cmap='viridis', alpha=0.6, s=50)\n",
                "axes[0].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
                "axes[0].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
                "axes[0].set_title(f'K-Means Clusters (k={best_k})', fontsize=14)\n",
                "plt.colorbar(scatter1, ax=axes[0])\n",
                "\n",
                "# Actual labels (ground truth)\n",
                "scatter2 = axes[1].scatter(X_pca[:, 0], X_pca[:, 1], c=df_engineered['is_good'], \n",
                "                          cmap='RdYlGn', alpha=0.6, s=50)\n",
                "axes[1].set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
                "axes[1].set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
                "axes[1].set_title('Actual Labels (Villain=Red, Hero=Green)', fontsize=14)\n",
                "plt.colorbar(scatter2, ax=axes[1])\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/clustering_pca_comparison.png', dpi=150)\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cluster profiling\n",
                "print(\"\\nCluster Profiles:\")\n",
                "cluster_profile = df_engineered.groupby('kmeans_cluster')[cluster_features + ['is_good']].mean()\n",
                "display(cluster_profile.round(2))\n",
                "\n",
                "# Name clusters based on characteristics\n",
                "def name_cluster(row):\n",
                "    parts = []\n",
                "    if row['power_level'] > cluster_profile['power_level'].median():\n",
                "        parts.append('High-Power')\n",
                "    else:\n",
                "        parts.append('Low-Power')\n",
                "    \n",
                "    if row['is_good'] > 0.5:\n",
                "        parts.append('Hero-Leaning')\n",
                "    else:\n",
                "        parts.append('Villain-Leaning')\n",
                "        \n",
                "    if row['civilian_casualties_past_year'] > cluster_profile['civilian_casualties_past_year'].median():\n",
                "        parts.append('Destructive')\n",
                "    else:\n",
                "        parts.append('Careful')\n",
                "    \n",
                "    return ' | '.join(parts)\n",
                "\n",
                "print(\"\\nCluster Archetypes:\")\n",
                "for idx, row in cluster_profile.iterrows():\n",
                "    name = name_cluster(row)\n",
                "    count = (df_engineered['kmeans_cluster'] == idx).sum()\n",
                "    print(f\"Cluster {idx} ({count} members): {name}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 9. Final Summary"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*80)\n",
                "print(\"FINAL PROJECT SUMMARY\")\n",
                "print(\"=\"*80)\n",
                "\n",
                "print(\"\\nüìä DATASET:\")\n",
                "print(f\"   ‚Ä¢ {len(df)} superheroes\")\n",
                "print(f\"   ‚Ä¢ {len(df.columns)} original features + {len(new_features)} engineered features\")\n",
                "print(f\"   ‚Ä¢ Target: 65% Heroes, 35% Villains\")\n",
                "\n",
                "print(\"\\nüî¨ MODELS TESTED:\")\n",
                "print(f\"   ‚Ä¢ {len(models)} different classification algorithms\")\n",
                "print(f\"   ‚Ä¢ Hyperparameter tuning for RF, GB, SVM\")\n",
                "print(f\"   ‚Ä¢ Voting and Stacking ensembles\")\n",
                "\n",
                "print(\"\\nüèÜ BEST CLASSIFICATION RESULTS:\")\n",
                "print(f\"   ‚Ä¢ Best Model: {best_result[0]}\")\n",
                "print(f\"   ‚Ä¢ Test Accuracy: {best_result[2]:.2%}\")\n",
                "print(f\"   ‚Ä¢ CV Accuracy: {best_result[1]:.2%}\")\n",
                "\n",
                "print(\"\\nüîç CLUSTERING RESULTS:\")\n",
                "print(f\"   ‚Ä¢ Best K-Means: k={best_k}, Silhouette={max(sil_scores):.4f}\")\n",
                "print(f\"   ‚Ä¢ Tested: K-Means, DBSCAN, Hierarchical\")\n",
                "\n",
                "print(\"\\nüí° KEY INSIGHTS:\")\n",
                "print(\"   1. Feature engineering improved model performance\")\n",
                "print(\"   2. Ensemble methods provided slight improvements\")\n",
                "print(f\"   3. Top features: {', '.join(feature_imp.tail(3)['feature'].tolist())}\")\n",
                "print(\"   4. Dataset has inherent limitations (~65-70% ceiling)\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save enhanced dataset with clusters\n",
                "df_engineered.to_csv('superhero_enhanced_clusters.csv', index=False)\n",
                "print(\"Enhanced dataset saved to 'superhero_enhanced_clusters.csv'\")\n",
                "\n",
                "# Save results summary\n",
                "results_df.to_csv('model_comparison_results.csv', index=False)\n",
                "print(\"Model results saved to 'model_comparison_results.csv'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}