{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Superhero Attributes and Power Classification\n",
                "\n",
                "**DSCI 4411 - Fundamentals of Data Mining**  \n",
                "**The American University in Cairo - Fall 2025**\n",
                "\n",
                "This project explores classification and clustering techniques using a dataset of superheroes with various attributes such as powers, skills, physical traits, and biography data."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup and Data Loading"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import required libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from sklearn.model_selection import train_test_split, cross_val_score\n",
                "from sklearn.preprocessing import StandardScaler\n",
                "from sklearn.linear_model import LogisticRegression\n",
                "from sklearn.ensemble import RandomForestClassifier\n",
                "from sklearn.svm import SVC\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
                "from sklearn.cluster import KMeans\n",
                "from sklearn.decomposition import PCA\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')\n",
                "\n",
                "# Set style for visualizations\n",
                "plt.style.use('seaborn-v0_8-whitegrid')\n",
                "sns.set_palette('husl')\n",
                "plt.rcParams['figure.figsize'] = (10, 6)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Load the dataset\n",
                "df = pd.read_csv('superhero dataset.csv')\n",
                "print(f\"Dataset Shape: {df.shape}\")\n",
                "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
                "df.head()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Exploratory Data Analysis (EDA)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset info and statistics\n",
                "print(\"Dataset Information:\")\n",
                "print(df.info())\n",
                "print(\"\\nStatistical Summary:\")\n",
                "df.describe()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Check for missing values\n",
                "print(\"Missing Values:\")\n",
                "print(df.isnull().sum())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Target variable distribution\n",
                "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
                "\n",
                "# Pie chart\n",
                "colors = ['#2ecc71', '#e74c3c']\n",
                "labels = ['Good (Hero)', 'Bad (Villain)']\n",
                "sizes = df['is_good'].value_counts().sort_index(ascending=False)\n",
                "axes[0].pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90, explode=(0.05, 0))\n",
                "axes[0].set_title('Hero vs Villain Distribution', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Bar chart\n",
                "ax = sns.countplot(data=df, x='is_good', palette=colors[::-1], ax=axes[1])\n",
                "axes[1].set_xticklabels(['Villain', 'Hero'])\n",
                "axes[1].set_xlabel('Character Type', fontsize=12)\n",
                "axes[1].set_ylabel('Count', fontsize=12)\n",
                "axes[1].set_title('Distribution of Heroes and Villains', fontsize=14, fontweight='bold')\n",
                "for p in ax.patches:\n",
                "    ax.annotate(f'{int(p.get_height())}', (p.get_x() + p.get_width() / 2., p.get_height()),\n",
                "                ha='center', va='bottom', fontsize=12)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/target_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create figures directory\n",
                "import os\n",
                "os.makedirs('figures', exist_ok=True)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Distribution of numerical features\n",
                "numerical_cols = ['height_cm', 'weight_kg', 'age', 'years_active', \n",
                "                  'training_hours_per_week', 'civilian_casualties_past_year',\n",
                "                  'power_level', 'public_approval_rating']\n",
                "\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, col in enumerate(numerical_cols):\n",
                "    sns.histplot(data=df, x=col, hue='is_good', kde=True, ax=axes[i], palette=colors[::-1])\n",
                "    axes[i].set_title(f'Distribution of {col}', fontsize=11)\n",
                "    axes[i].legend(['Villain', 'Hero'], title='Type')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/numerical_distributions.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Power distribution analysis\n",
                "power_cols = ['super_strength', 'flight', 'energy_projection', 'telepathy', \n",
                "              'healing_factor', 'shape_shifting', 'invisibility', 'telekinesis']\n",
                "\n",
                "power_counts = df[power_cols].sum().sort_values(ascending=True)\n",
                "\n",
                "plt.figure(figsize=(12, 6))\n",
                "colors_power = plt.cm.viridis(np.linspace(0.2, 0.8, len(power_counts)))\n",
                "bars = plt.barh(power_counts.index, power_counts.values, color=colors_power)\n",
                "plt.xlabel('Number of Superheroes', fontsize=12)\n",
                "plt.ylabel('Power Type', fontsize=12)\n",
                "plt.title('Distribution of Superpowers Across All Characters', fontsize=14, fontweight='bold')\n",
                "\n",
                "for bar, count in zip(bars, power_counts.values):\n",
                "    plt.text(count + 5, bar.get_y() + bar.get_height()/2, f'{count}', \n",
                "             va='center', fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/power_distribution.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Power comparison: Heroes vs Villains\n",
                "hero_powers = df[df['is_good'] == 1][power_cols].mean()\n",
                "villain_powers = df[df['is_good'] == 0][power_cols].mean()\n",
                "\n",
                "power_comparison = pd.DataFrame({\n",
                "    'Heroes': hero_powers * 100,\n",
                "    'Villains': villain_powers * 100\n",
                "})\n",
                "\n",
                "plt.figure(figsize=(12, 7))\n",
                "x = np.arange(len(power_cols))\n",
                "width = 0.35\n",
                "\n",
                "bars1 = plt.bar(x - width/2, power_comparison['Heroes'], width, label='Heroes', color='#2ecc71')\n",
                "bars2 = plt.bar(x + width/2, power_comparison['Villains'], width, label='Villains', color='#e74c3c')\n",
                "\n",
                "plt.xlabel('Power Type', fontsize=12)\n",
                "plt.ylabel('Percentage with Power (%)', fontsize=12)\n",
                "plt.title('Power Distribution: Heroes vs Villains', fontsize=14, fontweight='bold')\n",
                "plt.xticks(x, power_cols, rotation=45, ha='right')\n",
                "plt.legend()\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/hero_villain_powers.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Correlation heatmap\n",
                "plt.figure(figsize=(14, 12))\n",
                "correlation_matrix = df.corr()\n",
                "mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
                "\n",
                "sns.heatmap(correlation_matrix, mask=mask, annot=True, fmt='.2f', cmap='RdYlBu_r',\n",
                "            center=0, square=True, linewidths=0.5, cbar_kws={\"shrink\": 0.8})\n",
                "plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/correlation_heatmap.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Box plots comparing heroes vs villains for key metrics\n",
                "fig, axes = plt.subplots(2, 4, figsize=(16, 10))\n",
                "axes = axes.flatten()\n",
                "\n",
                "for i, col in enumerate(numerical_cols):\n",
                "    sns.boxplot(data=df, x='is_good', y=col, palette=colors[::-1], ax=axes[i])\n",
                "    axes[i].set_xticklabels(['Villain', 'Hero'])\n",
                "    axes[i].set_xlabel('')\n",
                "    axes[i].set_title(f'{col} by Type', fontsize=11)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/boxplots_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare features and target\n",
                "X = df.drop('is_good', axis=1)\n",
                "y = df['is_good']\n",
                "\n",
                "print(f\"Features shape: {X.shape}\")\n",
                "print(f\"Target shape: {y.shape}\")\n",
                "print(f\"\\nTarget class distribution:\")\n",
                "print(y.value_counts())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Split data into training and testing sets\n",
                "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
                "\n",
                "print(f\"Training set size: {len(X_train)}\")\n",
                "print(f\"Testing set size: {len(X_test)}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Scale numerical features for better model performance\n",
                "scaler = StandardScaler()\n",
                "X_train_scaled = scaler.fit_transform(X_train)\n",
                "X_test_scaled = scaler.transform(X_test)\n",
                "\n",
                "print(\"Feature scaling completed.\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Classification Models"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.1 Logistic Regression"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Logistic Regression\n",
                "lr_model = LogisticRegression(max_iter=1000, random_state=42)\n",
                "lr_model.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Predictions\n",
                "lr_pred = lr_model.predict(X_test_scaled)\n",
                "\n",
                "# Evaluation\n",
                "print(\"=\" * 50)\n",
                "print(\"LOGISTIC REGRESSION RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nAccuracy: {accuracy_score(y_test, lr_pred):.4f}\")\n",
                "print(f\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, lr_pred, target_names=['Villain', 'Hero']))\n",
                "\n",
                "# Cross-validation\n",
                "cv_scores_lr = cross_val_score(lr_model, X_train_scaled, y_train, cv=5)\n",
                "print(f\"Cross-Validation Accuracy: {cv_scores_lr.mean():.4f} (+/- {cv_scores_lr.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance for Logistic Regression\n",
                "feature_importance_lr = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'coefficient': np.abs(lr_model.coef_[0])\n",
                "}).sort_values('coefficient', ascending=True)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "colors_feat = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(feature_importance_lr)))\n",
                "plt.barh(feature_importance_lr['feature'], feature_importance_lr['coefficient'], color=colors_feat)\n",
                "plt.xlabel('Absolute Coefficient Value', fontsize=12)\n",
                "plt.ylabel('Feature', fontsize=12)\n",
                "plt.title('Logistic Regression: Feature Importance', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/lr_feature_importance.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.2 Random Forest Classifier"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Random Forest\n",
                "rf_model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
                "rf_model.fit(X_train, y_train)\n",
                "\n",
                "# Predictions\n",
                "rf_pred = rf_model.predict(X_test)\n",
                "\n",
                "# Evaluation\n",
                "print(\"=\" * 50)\n",
                "print(\"RANDOM FOREST RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nAccuracy: {accuracy_score(y_test, rf_pred):.4f}\")\n",
                "print(f\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, rf_pred, target_names=['Villain', 'Hero']))\n",
                "\n",
                "# Cross-validation\n",
                "cv_scores_rf = cross_val_score(rf_model, X_train, y_train, cv=5)\n",
                "print(f\"Cross-Validation Accuracy: {cv_scores_rf.mean():.4f} (+/- {cv_scores_rf.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Feature importance for Random Forest\n",
                "feature_importance_rf = pd.DataFrame({\n",
                "    'feature': X.columns,\n",
                "    'importance': rf_model.feature_importances_\n",
                "}).sort_values('importance', ascending=True)\n",
                "\n",
                "plt.figure(figsize=(10, 8))\n",
                "colors_feat = plt.cm.RdYlGn(np.linspace(0.2, 0.8, len(feature_importance_rf)))\n",
                "plt.barh(feature_importance_rf['feature'], feature_importance_rf['importance'], color=colors_feat)\n",
                "plt.xlabel('Feature Importance', fontsize=12)\n",
                "plt.ylabel('Feature', fontsize=12)\n",
                "plt.title('Random Forest: Feature Importance', fontsize=14, fontweight='bold')\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/rf_feature_importance.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.3 Support Vector Machine (SVM)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Support Vector Machine\n",
                "svm_model = SVC(kernel='rbf', C=1.0, random_state=42)\n",
                "svm_model.fit(X_train_scaled, y_train)\n",
                "\n",
                "# Predictions\n",
                "svm_pred = svm_model.predict(X_test_scaled)\n",
                "\n",
                "# Evaluation\n",
                "print(\"=\" * 50)\n",
                "print(\"SUPPORT VECTOR MACHINE RESULTS\")\n",
                "print(\"=\" * 50)\n",
                "print(f\"\\nAccuracy: {accuracy_score(y_test, svm_pred):.4f}\")\n",
                "print(f\"\\nClassification Report:\")\n",
                "print(classification_report(y_test, svm_pred, target_names=['Villain', 'Hero']))\n",
                "\n",
                "# Cross-validation\n",
                "cv_scores_svm = cross_val_score(svm_model, X_train_scaled, y_train, cv=5)\n",
                "print(f\"Cross-Validation Accuracy: {cv_scores_svm.mean():.4f} (+/- {cv_scores_svm.std() * 2:.4f})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 4.4 Model Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model comparison summary\n",
                "models_comparison = pd.DataFrame({\n",
                "    'Model': ['Logistic Regression', 'Random Forest', 'SVM'],\n",
                "    'Test Accuracy': [\n",
                "        accuracy_score(y_test, lr_pred),\n",
                "        accuracy_score(y_test, rf_pred),\n",
                "        accuracy_score(y_test, svm_pred)\n",
                "    ],\n",
                "    'CV Accuracy Mean': [cv_scores_lr.mean(), cv_scores_rf.mean(), cv_scores_svm.mean()],\n",
                "    'CV Accuracy Std': [cv_scores_lr.std(), cv_scores_rf.std(), cv_scores_svm.std()]\n",
                "})\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"MODEL COMPARISON SUMMARY\")\n",
                "print(\"=\" * 60)\n",
                "print(models_comparison.to_string(index=False))"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize model comparison\n",
                "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
                "\n",
                "# Bar chart for accuracy comparison\n",
                "colors_models = ['#3498db', '#2ecc71', '#e74c3c']\n",
                "x = np.arange(len(models_comparison))\n",
                "axes[0].bar(x, models_comparison['Test Accuracy'], color=colors_models)\n",
                "axes[0].set_xticks(x)\n",
                "axes[0].set_xticklabels(models_comparison['Model'])\n",
                "axes[0].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[0].set_title('Test Accuracy Comparison', fontsize=14, fontweight='bold')\n",
                "axes[0].set_ylim(0, 1)\n",
                "for i, v in enumerate(models_comparison['Test Accuracy']):\n",
                "    axes[0].text(i, v + 0.02, f'{v:.3f}', ha='center', fontsize=11)\n",
                "\n",
                "# Cross-validation with error bars\n",
                "axes[1].bar(x, models_comparison['CV Accuracy Mean'], \n",
                "            yerr=models_comparison['CV Accuracy Std'] * 2,\n",
                "            color=colors_models, capsize=5)\n",
                "axes[1].set_xticks(x)\n",
                "axes[1].set_xticklabels(models_comparison['Model'])\n",
                "axes[1].set_ylabel('Accuracy', fontsize=12)\n",
                "axes[1].set_title('Cross-Validation Accuracy (with 95% CI)', fontsize=14, fontweight='bold')\n",
                "axes[1].set_ylim(0, 1)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/model_comparison.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Confusion matrices for all models\n",
                "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
                "\n",
                "predictions = [lr_pred, rf_pred, svm_pred]\n",
                "model_names = ['Logistic Regression', 'Random Forest', 'SVM']\n",
                "\n",
                "for i, (pred, name) in enumerate(zip(predictions, model_names)):\n",
                "    cm = confusion_matrix(y_test, pred)\n",
                "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[i],\n",
                "                xticklabels=['Villain', 'Hero'], yticklabels=['Villain', 'Hero'])\n",
                "    axes[i].set_xlabel('Predicted', fontsize=11)\n",
                "    axes[i].set_ylabel('Actual', fontsize=11)\n",
                "    axes[i].set_title(f'{name}', fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.suptitle('Confusion Matrices', fontsize=14, fontweight='bold', y=1.02)\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/confusion_matrices.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Clustering Analysis"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.1 Finding Optimal Number of Clusters"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Prepare data for clustering (using all features)\n",
                "X_cluster = scaler.fit_transform(X)\n",
                "\n",
                "# Elbow method to find optimal k\n",
                "inertias = []\n",
                "K_range = range(2, 11)\n",
                "\n",
                "for k in K_range:\n",
                "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
                "    kmeans.fit(X_cluster)\n",
                "    inertias.append(kmeans.inertia_)\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(K_range, inertias, 'bo-', linewidth=2, markersize=8)\n",
                "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
                "plt.ylabel('Inertia (Within-cluster Sum of Squares)', fontsize=12)\n",
                "plt.title('Elbow Method for Optimal k', fontsize=14, fontweight='bold')\n",
                "plt.xticks(K_range)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig('figures/elbow_method.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Silhouette score analysis\n",
                "from sklearn.metrics import silhouette_score\n",
                "\n",
                "silhouette_scores = []\n",
                "for k in K_range:\n",
                "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
                "    labels = kmeans.fit_predict(X_cluster)\n",
                "    silhouette_scores.append(silhouette_score(X_cluster, labels))\n",
                "\n",
                "plt.figure(figsize=(10, 6))\n",
                "plt.plot(K_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
                "plt.xlabel('Number of Clusters (k)', fontsize=12)\n",
                "plt.ylabel('Silhouette Score', fontsize=12)\n",
                "plt.title('Silhouette Score for Different k Values', fontsize=14, fontweight='bold')\n",
                "plt.xticks(K_range)\n",
                "plt.grid(True, alpha=0.3)\n",
                "plt.savefig('figures/silhouette_scores.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "optimal_k = K_range[np.argmax(silhouette_scores)]\n",
                "print(f\"\\nOptimal number of clusters based on Silhouette Score: {optimal_k}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### 5.2 K-Means Clustering with Optimal k"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Perform K-Means clustering with optimal k (using k=4 for interpretable archetypes)\n",
                "n_clusters = 4  # Using 4 clusters for interesting superhero archetypes\n",
                "\n",
                "kmeans_final = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
                "cluster_labels = kmeans_final.fit_predict(X_cluster)\n",
                "\n",
                "# Add cluster labels to dataframe\n",
                "df['cluster'] = cluster_labels\n",
                "\n",
                "print(f\"Cluster Distribution:\")\n",
                "print(df['cluster'].value_counts().sort_index())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# PCA for visualization\n",
                "pca = PCA(n_components=2)\n",
                "X_pca = pca.fit_transform(X_cluster)\n",
                "\n",
                "plt.figure(figsize=(12, 8))\n",
                "scatter = plt.scatter(X_pca[:, 0], X_pca[:, 1], c=cluster_labels, \n",
                "                      cmap='viridis', alpha=0.7, s=50)\n",
                "plt.colorbar(scatter, label='Cluster')\n",
                "plt.xlabel(f'First Principal Component ({pca.explained_variance_ratio_[0]*100:.1f}% variance)', fontsize=12)\n",
                "plt.ylabel(f'Second Principal Component ({pca.explained_variance_ratio_[1]*100:.1f}% variance)', fontsize=12)\n",
                "plt.title('K-Means Clustering: PCA Visualization', fontsize=14, fontweight='bold')\n",
                "\n",
                "# Plot cluster centers\n",
                "centers_pca = pca.transform(kmeans_final.cluster_centers_)\n",
                "plt.scatter(centers_pca[:, 0], centers_pca[:, 1], c='red', marker='X', \n",
                "            s=200, edgecolors='black', linewidths=2, label='Cluster Centers')\n",
                "plt.legend()\n",
                "plt.savefig('figures/clustering_pca.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()\n",
                "\n",
                "print(f\"\\nTotal variance explained by 2 PCs: {sum(pca.explained_variance_ratio_)*100:.2f}%\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Cluster profiling\n",
                "cluster_profiles = df.groupby('cluster').mean()\n",
                "\n",
                "print(\"\\n\" + \"=\" * 60)\n",
                "print(\"CLUSTER PROFILES\")\n",
                "print(\"=\" * 60)\n",
                "\n",
                "for cluster in range(n_clusters):\n",
                "    print(f\"\\n--- Cluster {cluster} ({len(df[df['cluster']==cluster])} members) ---\")\n",
                "    profile = cluster_profiles.loc[cluster]\n",
                "    print(f\"  Average Power Level: {profile['power_level']:.1f}\")\n",
                "    print(f\"  Public Approval: {profile['public_approval_rating']:.1f}%\")\n",
                "    print(f\"  % Heroes: {profile['is_good']*100:.1f}%\")\n",
                "    print(f\"  Training Hours/Week: {profile['training_hours_per_week']:.1f}\")\n",
                "    print(f\"  Civilian Casualties: {profile['civilian_casualties_past_year']:.2f}\")\n",
                "    \n",
                "    # Top powers\n",
                "    power_profile = profile[power_cols].sort_values(ascending=False)\n",
                "    print(f\"  Top Powers: {', '.join(power_profile.head(3).index.tolist())}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualize cluster characteristics\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
                "\n",
                "# Plot 1: Average stats by cluster\n",
                "stats_cols = ['power_level', 'public_approval_rating', 'training_hours_per_week']\n",
                "cluster_stats = cluster_profiles[stats_cols]\n",
                "cluster_stats.plot(kind='bar', ax=axes[0, 0], colormap='viridis')\n",
                "axes[0, 0].set_title('Average Stats by Cluster', fontsize=12, fontweight='bold')\n",
                "axes[0, 0].set_xlabel('Cluster')\n",
                "axes[0, 0].set_ylabel('Value')\n",
                "axes[0, 0].legend(loc='upper right')\n",
                "axes[0, 0].tick_params(axis='x', rotation=0)\n",
                "\n",
                "# Plot 2: Hero ratio by cluster\n",
                "hero_ratio = cluster_profiles['is_good'] * 100\n",
                "colors_clusters = plt.cm.viridis(np.linspace(0.2, 0.8, n_clusters))\n",
                "axes[0, 1].bar(range(n_clusters), hero_ratio, color=colors_clusters)\n",
                "axes[0, 1].set_title('Percentage of Heroes by Cluster', fontsize=12, fontweight='bold')\n",
                "axes[0, 1].set_xlabel('Cluster')\n",
                "axes[0, 1].set_ylabel('% Heroes')\n",
                "axes[0, 1].set_xticks(range(n_clusters))\n",
                "\n",
                "# Plot 3: Power distribution by cluster\n",
                "power_by_cluster = cluster_profiles[power_cols].T\n",
                "power_by_cluster.plot(kind='bar', ax=axes[1, 0], colormap='viridis')\n",
                "axes[1, 0].set_title('Power Distribution by Cluster', fontsize=12, fontweight='bold')\n",
                "axes[1, 0].set_xlabel('Power Type')\n",
                "axes[1, 0].set_ylabel('Proportion')\n",
                "axes[1, 0].tick_params(axis='x', rotation=45)\n",
                "axes[1, 0].legend(title='Cluster', loc='upper right')\n",
                "\n",
                "# Plot 4: Cluster size\n",
                "cluster_sizes = df['cluster'].value_counts().sort_index()\n",
                "axes[1, 1].pie(cluster_sizes, labels=[f'Cluster {i}' for i in range(n_clusters)],\n",
                "               autopct='%1.1f%%', colors=colors_clusters, startangle=90)\n",
                "axes[1, 1].set_title('Cluster Size Distribution', fontsize=12, fontweight='bold')\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('figures/cluster_analysis.png', dpi=300, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Assign archetype names based on cluster characteristics\n",
                "archetype_names = {}\n",
                "for cluster in range(n_clusters):\n",
                "    profile = cluster_profiles.loc[cluster]\n",
                "    is_hero = profile['is_good'] > 0.5\n",
                "    power = profile['power_level']\n",
                "    approval = profile['public_approval_rating']\n",
                "    \n",
                "    if is_hero and power > 60 and approval > 60:\n",
                "        archetype_names[cluster] = \"Public Hero\"\n",
                "    elif is_hero and power > 60:\n",
                "        archetype_names[cluster] = \"Vigilante Hero\"\n",
                "    elif not is_hero and power > 60:\n",
                "        archetype_names[cluster] = \"Supervillain\"\n",
                "    elif is_hero:\n",
                "        archetype_names[cluster] = \"Street-Level Hero\"\n",
                "    else:\n",
                "        archetype_names[cluster] = \"Common Villain\"\n",
                "\n",
                "print(\"\\nSuperhero Archetypes Discovered:\")\n",
                "for cluster, name in archetype_names.items():\n",
                "    count = len(df[df['cluster'] == cluster])\n",
                "    print(f\"  Cluster {cluster}: {name} ({count} members)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 6. Summary and Conclusions"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(\"=\"*70)\n",
                "print(\"PROJECT SUMMARY: SUPERHERO ATTRIBUTES AND POWER CLASSIFICATION\")\n",
                "print(\"=\"*70)\n",
                "\n",
                "print(\"\\nüìä DATASET OVERVIEW:\")\n",
                "print(f\"   ‚Ä¢ Total superheroes: {len(df)}\")\n",
                "print(f\"   ‚Ä¢ Features: 16 (8 numerical, 8 binary power flags)\")\n",
                "print(f\"   ‚Ä¢ Target: is_good (65% heroes, 35% villains)\")\n",
                "\n",
                "print(\"\\nüéØ CLASSIFICATION RESULTS:\")\n",
                "best_model = models_comparison.loc[models_comparison['Test Accuracy'].idxmax()]\n",
                "print(f\"   ‚Ä¢ Best Model: {best_model['Model']}\")\n",
                "print(f\"   ‚Ä¢ Test Accuracy: {best_model['Test Accuracy']:.2%}\")\n",
                "print(f\"   ‚Ä¢ Cross-Validation Accuracy: {best_model['CV Accuracy Mean']:.2%} (¬±{best_model['CV Accuracy Std']*2:.2%})\")\n",
                "\n",
                "print(\"\\nüîç KEY FEATURES FOR HERO/VILLAIN CLASSIFICATION:\")\n",
                "top_features = feature_importance_rf.tail(5)\n",
                "for _, row in top_features.iterrows():\n",
                "    print(f\"   ‚Ä¢ {row['feature']}: {row['importance']:.3f}\")\n",
                "\n",
                "print(\"\\nüë• CLUSTERING ANALYSIS:\")\n",
                "print(f\"   ‚Ä¢ Number of clusters (archetypes): {n_clusters}\")\n",
                "print(f\"   ‚Ä¢ Silhouette Score: {silhouette_score(X_cluster, cluster_labels):.3f}\")\n",
                "print(\"\\n   Archetypes discovered:\")\n",
                "for cluster, name in archetype_names.items():\n",
                "    count = len(df[df['cluster'] == cluster])\n",
                "    pct = count / len(df) * 100\n",
                "    print(f\"   ‚Ä¢ {name}: {count} ({pct:.1f}%)\")\n",
                "\n",
                "print(\"\\nüí° KEY INSIGHTS:\")\n",
                "print(\"   1. Public approval and power level are key differentiators between heroes and villains\")\n",
                "print(\"   2. Civilian casualties correlate with villain classification\")\n",
                "print(\"   3. Training hours show similar distributions across both groups\")\n",
                "print(\"   4. Superpowers alone don't determine hero/villain status - behavioral factors matter more\")\n",
                "print(\"\\n\" + \"=\"*70)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save the final dataframe with cluster assignments\n",
                "df.to_csv('superhero_with_clusters.csv', index=False)\n",
                "print(\"Enhanced dataset saved to 'superhero_with_clusters.csv'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.10.0"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}